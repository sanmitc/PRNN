{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5e5d00",
   "metadata": {},
   "source": [
    "# BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff60a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc46593",
   "metadata": {},
   "source": [
    "we define the error back propagation with respect to the actual target y and the predicted target h\n",
    "\n",
    "\n",
    "Here the target and the prediction are a martix. \n",
    "\n",
    "Assumptions about the original function, the weight vectors are stored in an 1-D array.\n",
    "having N elements, each element being an array itself having as many elements as the number of neurons in that layer.\n",
    "\n",
    "\n",
    "\n",
    "In this case we take, sigmoid activation function, although we define other functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75ea06",
   "metadata": {},
   "source": [
    "### Defining the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activations:\n",
    "    class sigmoid:\n",
    "        def eval(z):\n",
    "            return 1/(1+math.exp(-z))\n",
    "        def derivative(z):\n",
    "            return math.exp(z)/(1+math.exp(-z))**2\n",
    "        \n",
    "    class tanh:\n",
    "        def eval(z):\n",
    "            return math.tanh(z)\n",
    "        def derivative(z):\n",
    "            return 1/(math.cosh(z))**2\n",
    "        \n",
    "    class leaky_ReLU:\n",
    "        def __init__(slope_n=1e-3):\n",
    "            self.slope_n=slope_n\n",
    "            \n",
    "        def eval(z):\n",
    "            if(z>=0):\n",
    "                return z\n",
    "            else:\n",
    "                return slope_n*z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc8d6b",
   "metadata": {},
   "source": [
    "### defining a layer in the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bad9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(Input, Out, activation=\"None\"):\n",
    "        self.Input=Input\n",
    "        self.In=Input.shape[0]\n",
    "        self.Out=Out\n",
    "        self.activation=activation\n",
    "        #random initialisation of weight matrix and biases\n",
    "        self.weight=np.random.randn(Out, In)\n",
    "        self.bias=np.random.randn(Out,1)\n",
    "    \n",
    "    def f_prop:\n",
    "        return activations.sigmoid(np.dot(self.weight,self.Input))\n",
    "    \n",
    "    def backprop(learning rate, output_derivative):\n",
    "         #we divide the whole set-up into two parts, one is the weigted sum layer and the other is \n",
    "         #the other one is the activation layer.\n",
    "        if(activation=\"None\"):\n",
    "            self.bias = self.bias - output_derivative\n",
    "            self.weight = self.weight - np.dot(output_derivative,Input.T)\n",
    "            output_derivative= np.dot(self.weight.T, output_derivative)\n",
    "        if(activation=\"sigmoid\"):\n",
    "            output_derivative= np.multiply(output_derivative, activations.sigmoid.eval(Input)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
